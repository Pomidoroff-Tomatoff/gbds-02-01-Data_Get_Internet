GB BigData / [Олег Гладкий | Oleg Gladkiy](https://gb.ru/users/3837199) // домашнее задание
Lesson-06. Scrapy: piplines + splash.


Проект: splash_quotes

   -- Цитаты и книги учебного сайта

   Сайт:   quotes.toscrape.com./js

   Пауки:

      author.py -- шаблон основной, заходим на страницу каждого автора + MongoDB

         scrapy crawl author

         -- парсим краткий список учебного сайта "quotes.toscrape.com" для получения ссылок страниц авторов
         -- ныряем в станички автора и парсим их, без повторений
         -- проходим по всем страницам кратного списка (50 стр.)
         -- пишем полученные данных в базу MongoDB
            > имя коллекции определяется по атрибуту collection_name 
            из класса данных Author_SplashQuotesItem(scrapy.Item)
         Прим.: 
            данамический аналог этого сайта не используется, так как не содержит ссылок на авторов...


      quotes.py -- шаблон основной, динамический сайт, acrapy + SPLASH + MongoDB

         scrapy crawl quotes

         -- парсим краткий список учебного динамического сайта "quotes.toscrape.com/js"
         -- проваливаемся по полученным ссылкам (на индивидуальные страницы) 
            и получаем индивидуальные страницы используя метод SplashRequest(...)
         -- заносим парснутые данные с каждой страницы в базу данны MongoDB
            > имя коллекции определяется по атрибуту collection_name 
            из класса данных Quote_SplashQuotesItem(scrapy.Item)


      quotes_author.py -- шаблон основной

         scrapy crawl pages quotes_author

         -- объединение обоих пауков в один
         -- сайт парсим не динамический, так как последний не содержит ссылок на страницы авторов...


ПРОЕКТ: job
   -- Сайты работ

   Пауки:

      hh_list.py (job.spiders.hh), паук
         scrapy crawl hh_list
         https://hh.ru/search/vacancy/

         -- Краткий список вакансий, не заходим на индивидуальную страничку вакансии
         -- Дубликаты не фильтруются
         -- Pipelines.ru, базы: 
            MongoDB: job/vacancies_list
               > имя базы задаётся как атрибут класса в pipelines.py
               > имя коллекции задаётся как обычный атрибут класса типа item (уникальный для паука)
            SQLite3: job_base__hh_pages.db/vacancies
               > Имя базы определяется по имени паука классом обработки в piplines.py
               > имя таблицы определяется атрибутом класса pipelines.py -- одинаковое для всех пауков

      hh_pages.py (job.spiders.hh), паук
         scrapy crawl hh_pages
         https://hh.ru/search/vacancy/

         -- Проходя по краткому списку заходим на индивидуальные странички вакансий
         -- Дубликаты фильтруются Scrapy (как-то, я включил это в настройках)
         -- Возможность использовая Splash включается атрибутом класса "splash_mode"
            > сайт статический, так что splash не нужен, но можно включить.
         -- Pipelines.ru, базы: 
            MongoDB: job/vacancies_list
               > имя базы задаётся как атрибут класса в pipelines.py
               > имя коллекции задаётся как обычный атрибут класса типа item (уникальный для паука)
            SQLite3: job_base__hh_pages.db/vacancies
               > Имя базы определяется по имени паука классом обработки в piplines.py
               > имя таблицы определяется атрибутом класса pipelines.py -- одинаковое для всех пауков
